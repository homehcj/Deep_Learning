{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1538,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成数据集 \n",
    "y= **Xw** + b + c(噪声满足正态分布) \n",
    "+ _n个样本数据集，2个特征_\n",
    "+ w=[25. -30.4] b=1000.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1539,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(n,w,b):\n",
    "    X=torch.normal(0,1,(n,len(w)))\n",
    "    y=torch.matmul(X,w) + b\n",
    "    y=y+torch.normal(0,0.01,y.shape)\n",
    "    return X,y.reshape((-1,1))\n",
    "\n",
    "x,y=create_data(1000,torch.tensor([1.2,-1.4]),0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 绘图函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polt_style(axlim=[0,10],color='bule', title='Figure',ylabel='Y', xlabel='X',subplot=111):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(subplot)\n",
    "    ax.set(xlim=axlim, ylim=axlim,title=title,ylabel=ylabel, xlabel=xlabel)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1f49eb59550>"
      ]
     },
     "execution_count": 1541,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "polt_style(axlim=[-10,15],subplot=221).scatter(x[:, 1].detach().numpy(), y.detach().numpy(), 1)\n",
    "polt_style(axlim=[-10,15],subplot=222).scatter(x[:, 0].detach().numpy(), y.detach().numpy(), 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取数据集\n",
    "每次抽取⼀⼩批量样本，并使⽤它们来更新我们的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_minexamples(num,x,y):\n",
    "    #随机化抽样序列\n",
    "    array=list(range(len(x)))\n",
    "    random.shuffle(array)\n",
    "    #抽样\n",
    "    for i in range(0,len(x),num):\n",
    "        #每次抽样num个\n",
    "        getarray = torch.tensor(\n",
    "        array[i:min(i + num, len(x))])\n",
    "        #迭代\n",
    "        yield x[getarray],y[getarray]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化模型参数\n",
    "+ 初始化权重w\n",
    "+ 初始化偏置b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0169],\n",
      "        [0.0086]], requires_grad=True) tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "print(w,b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line(w,x,b):\n",
    "    return torch.matmul(x,w)+b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义损失函数\n",
    "+ 平方差损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_guess,y):\n",
    "    #print(y_guess,y,1/2*((y_guess-y)**2).sum())\n",
    "    return (y_guess-y.reshape(y_guess.shape))**2/2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 反向传播梯度，\n",
    "+ 小批量随机梯度下降\n",
    "+ 学习速率learnr\n",
    "+ p需要偏导参数\n",
    "+ num步长，抽样数量\n",
    "+ **-=** **如果不这样处理偏导会消失**，无法进行梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1546,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(p, learnr, num):\n",
    "    with torch.no_grad():\n",
    "        for pi in p:\n",
    "            pi -= learnr * pi.grad / num\n",
    "            pi.grad.zero_()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学习\n",
    "1. 初始化参数\n",
    "2. 重复训练\n",
    "    1. 计算梯度\n",
    "    2. 更新权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1547,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学习率\n",
    "learnr=0.03\n",
    "#循环训练次数\n",
    "learn_num=100\n",
    "#抽样个数\n",
    "minnum = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num 1, loss 0.004288\n",
      "tensor([[ 1.1466],\n",
      "        [-1.3230]], requires_grad=True) \t tensor([0.1954], requires_grad=True)\n",
      "num 2, loss 0.000058\n",
      "tensor([[ 1.1974],\n",
      "        [-1.3952]], requires_grad=True) \t tensor([0.2002], requires_grad=True)\n",
      "num 3, loss 0.000045\n",
      "tensor([[ 1.1996],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.2000], requires_grad=True)\n",
      "num 4, loss 0.000046\n",
      "tensor([[ 1.1995],\n",
      "        [-1.3996]], requires_grad=True) \t tensor([0.2004], requires_grad=True)\n",
      "num 5, loss 0.000045\n",
      "tensor([[ 1.1999],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.2002], requires_grad=True)\n",
      "num 6, loss 0.000045\n",
      "tensor([[ 1.2003],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.1999], requires_grad=True)\n",
      "num 7, loss 0.000046\n",
      "tensor([[ 1.2004],\n",
      "        [-1.3994]], requires_grad=True) \t tensor([0.2003], requires_grad=True)\n",
      "num 8, loss 0.000046\n",
      "tensor([[ 1.2004],\n",
      "        [-1.3996]], requires_grad=True) \t tensor([0.2003], requires_grad=True)\n",
      "num 9, loss 0.000045\n",
      "tensor([[ 1.2001],\n",
      "        [-1.3996]], requires_grad=True) \t tensor([0.1999], requires_grad=True)\n",
      "num 10, loss 0.000046\n",
      "tensor([[ 1.2000],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.1996], requires_grad=True)\n",
      "num 11, loss 0.000045\n",
      "tensor([[ 1.1997],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.1999], requires_grad=True)\n",
      "num 12, loss 0.000046\n",
      "tensor([[ 1.2001],\n",
      "        [-1.3999]], requires_grad=True) \t tensor([0.1998], requires_grad=True)\n",
      "num 13, loss 0.000046\n",
      "tensor([[ 1.1998],\n",
      "        [-1.4000]], requires_grad=True) \t tensor([0.1998], requires_grad=True)\n",
      "num 14, loss 0.000045\n",
      "tensor([[ 1.1998],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.2003], requires_grad=True)\n",
      "num 15, loss 0.000045\n",
      "tensor([[ 1.1998],\n",
      "        [-1.3999]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n",
      "num 16, loss 0.000045\n",
      "tensor([[ 1.2002],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n",
      "num 17, loss 0.000046\n",
      "tensor([[ 1.1997],\n",
      "        [-1.3996]], requires_grad=True) \t tensor([0.1997], requires_grad=True)\n",
      "num 18, loss 0.000045\n",
      "tensor([[ 1.1997],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.1999], requires_grad=True)\n",
      "num 19, loss 0.000045\n",
      "tensor([[ 1.1997],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.2002], requires_grad=True)\n",
      "num 20, loss 0.000046\n",
      "tensor([[ 1.2000],\n",
      "        [-1.3993]], requires_grad=True) \t tensor([0.2003], requires_grad=True)\n",
      "num 21, loss 0.000046\n",
      "tensor([[ 1.2005],\n",
      "        [-1.3998]], requires_grad=True) \t tensor([0.1999], requires_grad=True)\n",
      "num 22, loss 0.000045\n",
      "tensor([[ 1.2000],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.2002], requires_grad=True)\n",
      "num 23, loss 0.000046\n",
      "tensor([[ 1.1995],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.2002], requires_grad=True)\n",
      "num 24, loss 0.000046\n",
      "tensor([[ 1.1994],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.2000], requires_grad=True)\n",
      "num 25, loss 0.000045\n",
      "tensor([[ 1.1998],\n",
      "        [-1.3994]], requires_grad=True) \t tensor([0.2002], requires_grad=True)\n",
      "num 26, loss 0.000045\n",
      "tensor([[ 1.1999],\n",
      "        [-1.3998]], requires_grad=True) \t tensor([0.2000], requires_grad=True)\n",
      "num 27, loss 0.000045\n",
      "tensor([[ 1.1997],\n",
      "        [-1.3996]], requires_grad=True) \t tensor([0.2003], requires_grad=True)\n",
      "num 28, loss 0.000045\n",
      "tensor([[ 1.1998],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.1999], requires_grad=True)\n",
      "num 29, loss 0.000045\n",
      "tensor([[ 1.2001],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.1999], requires_grad=True)\n",
      "num 30, loss 0.000045\n",
      "tensor([[ 1.2001],\n",
      "        [-1.3998]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n",
      "num 31, loss 0.000045\n",
      "tensor([[ 1.1999],\n",
      "        [-1.3999]], requires_grad=True) \t tensor([0.2003], requires_grad=True)\n",
      "num 32, loss 0.000046\n",
      "tensor([[ 1.1994],\n",
      "        [-1.4002]], requires_grad=True) \t tensor([0.2002], requires_grad=True)\n",
      "num 33, loss 0.000045\n",
      "tensor([[ 1.2002],\n",
      "        [-1.3998]], requires_grad=True) \t tensor([0.1999], requires_grad=True)\n",
      "num 34, loss 0.000046\n",
      "tensor([[ 1.1996],\n",
      "        [-1.4001]], requires_grad=True) \t tensor([0.2000], requires_grad=True)\n",
      "num 35, loss 0.000046\n",
      "tensor([[ 1.1996],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.2002], requires_grad=True)\n",
      "num 36, loss 0.000046\n",
      "tensor([[ 1.2004],\n",
      "        [-1.4001]], requires_grad=True) \t tensor([0.2002], requires_grad=True)\n",
      "num 37, loss 0.000046\n",
      "tensor([[ 1.2002],\n",
      "        [-1.3994]], requires_grad=True) \t tensor([0.1999], requires_grad=True)\n",
      "num 38, loss 0.000045\n",
      "tensor([[ 1.2002],\n",
      "        [-1.3996]], requires_grad=True) \t tensor([0.2000], requires_grad=True)\n",
      "num 39, loss 0.000045\n",
      "tensor([[ 1.2000],\n",
      "        [-1.3998]], requires_grad=True) \t tensor([0.1999], requires_grad=True)\n",
      "num 40, loss 0.000046\n",
      "tensor([[ 1.2003],\n",
      "        [-1.3994]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n",
      "num 41, loss 0.000046\n",
      "tensor([[ 1.1995],\n",
      "        [-1.3998]], requires_grad=True) \t tensor([0.2002], requires_grad=True)\n",
      "num 42, loss 0.000045\n",
      "tensor([[ 1.2000],\n",
      "        [-1.3999]], requires_grad=True) \t tensor([0.2003], requires_grad=True)\n",
      "num 43, loss 0.000046\n",
      "tensor([[ 1.2000],\n",
      "        [-1.3992]], requires_grad=True) \t tensor([0.2000], requires_grad=True)\n",
      "num 44, loss 0.000045\n",
      "tensor([[ 1.1998],\n",
      "        [-1.3994]], requires_grad=True) \t tensor([0.2000], requires_grad=True)\n",
      "num 45, loss 0.000046\n",
      "tensor([[ 1.1996],\n",
      "        [-1.4000]], requires_grad=True) \t tensor([0.2003], requires_grad=True)\n",
      "num 46, loss 0.000046\n",
      "tensor([[ 1.1995],\n",
      "        [-1.3998]], requires_grad=True) \t tensor([0.1996], requires_grad=True)\n",
      "num 47, loss 0.000045\n",
      "tensor([[ 1.2003],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.1999], requires_grad=True)\n",
      "num 48, loss 0.000046\n",
      "tensor([[ 1.1996],\n",
      "        [-1.3994]], requires_grad=True) \t tensor([0.2005], requires_grad=True)\n",
      "num 49, loss 0.000045\n",
      "tensor([[ 1.1997],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.2000], requires_grad=True)\n",
      "num 50, loss 0.000045\n",
      "tensor([[ 1.1998],\n",
      "        [-1.3998]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n",
      "num 51, loss 0.000045\n",
      "tensor([[ 1.1998],\n",
      "        [-1.3996]], requires_grad=True) \t tensor([0.2002], requires_grad=True)\n",
      "num 52, loss 0.000045\n",
      "tensor([[ 1.1999],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.2003], requires_grad=True)\n",
      "num 53, loss 0.000045\n",
      "tensor([[ 1.2003],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.2000], requires_grad=True)\n",
      "num 54, loss 0.000045\n",
      "tensor([[ 1.2003],\n",
      "        [-1.3996]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n",
      "num 55, loss 0.000045\n",
      "tensor([[ 1.1999],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n",
      "num 56, loss 0.000045\n",
      "tensor([[ 1.2002],\n",
      "        [-1.3999]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n",
      "num 57, loss 0.000045\n",
      "tensor([[ 1.1999],\n",
      "        [-1.3996]], requires_grad=True) \t tensor([0.2000], requires_grad=True)\n",
      "num 58, loss 0.000045\n",
      "tensor([[ 1.1998],\n",
      "        [-1.3999]], requires_grad=True) \t tensor([0.2000], requires_grad=True)\n",
      "num 59, loss 0.000046\n",
      "tensor([[ 1.2003],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n",
      "num 60, loss 0.000045\n",
      "tensor([[ 1.2000],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.2004], requires_grad=True)\n",
      "num 61, loss 0.000045\n",
      "tensor([[ 1.1999],\n",
      "        [-1.3998]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n",
      "num 62, loss 0.000046\n",
      "tensor([[ 1.2003],\n",
      "        [-1.3998]], requires_grad=True) \t tensor([0.1998], requires_grad=True)\n",
      "num 63, loss 0.000046\n",
      "tensor([[ 1.2003],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.2005], requires_grad=True)\n",
      "num 64, loss 0.000046\n",
      "tensor([[ 1.2001],\n",
      "        [-1.3996]], requires_grad=True) \t tensor([0.1995], requires_grad=True)\n",
      "num 65, loss 0.000046\n",
      "tensor([[ 1.2001],\n",
      "        [-1.3999]], requires_grad=True) \t tensor([0.1999], requires_grad=True)\n",
      "num 66, loss 0.000045\n",
      "tensor([[ 1.1999],\n",
      "        [-1.3999]], requires_grad=True) \t tensor([0.1998], requires_grad=True)\n",
      "num 67, loss 0.000045\n",
      "tensor([[ 1.1999],\n",
      "        [-1.3994]], requires_grad=True) \t tensor([0.2000], requires_grad=True)\n",
      "num 68, loss 0.000045\n",
      "tensor([[ 1.2000],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.2000], requires_grad=True)\n",
      "num 69, loss 0.000045\n",
      "tensor([[ 1.2001],\n",
      "        [-1.3994]], requires_grad=True) \t tensor([0.1998], requires_grad=True)\n",
      "num 70, loss 0.000046\n",
      "tensor([[ 1.1995],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.2003], requires_grad=True)\n",
      "num 71, loss 0.000046\n",
      "tensor([[ 1.1995],\n",
      "        [-1.3993]], requires_grad=True) \t tensor([0.1998], requires_grad=True)\n",
      "num 72, loss 0.000045\n",
      "tensor([[ 1.1998],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n",
      "num 73, loss 0.000045\n",
      "tensor([[ 1.1998],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.2002], requires_grad=True)\n",
      "num 74, loss 0.000046\n",
      "tensor([[ 1.1998],\n",
      "        [-1.3998]], requires_grad=True) \t tensor([0.1995], requires_grad=True)\n",
      "num 75, loss 0.000046\n",
      "tensor([[ 1.2001],\n",
      "        [-1.3993]], requires_grad=True) \t tensor([0.2004], requires_grad=True)\n",
      "num 76, loss 0.000045\n",
      "tensor([[ 1.2001],\n",
      "        [-1.3999]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n",
      "num 77, loss 0.000046\n",
      "tensor([[ 1.2003],\n",
      "        [-1.3994]], requires_grad=True) \t tensor([0.1997], requires_grad=True)\n",
      "num 78, loss 0.000045\n",
      "tensor([[ 1.2000],\n",
      "        [-1.3993]], requires_grad=True) \t tensor([0.1999], requires_grad=True)\n",
      "num 79, loss 0.000045\n",
      "tensor([[ 1.2000],\n",
      "        [-1.3994]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n",
      "num 80, loss 0.000045\n",
      "tensor([[ 1.1998],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.1998], requires_grad=True)\n",
      "num 81, loss 0.000046\n",
      "tensor([[ 1.2005],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.2006], requires_grad=True)\n",
      "num 82, loss 0.000046\n",
      "tensor([[ 1.1997],\n",
      "        [-1.3996]], requires_grad=True) \t tensor([0.1996], requires_grad=True)\n",
      "num 83, loss 0.000046\n",
      "tensor([[ 1.1997],\n",
      "        [-1.4001]], requires_grad=True) \t tensor([0.1997], requires_grad=True)\n",
      "num 84, loss 0.000046\n",
      "tensor([[ 1.1998],\n",
      "        [-1.3996]], requires_grad=True) \t tensor([0.1994], requires_grad=True)\n",
      "num 85, loss 0.000045\n",
      "tensor([[ 1.2001],\n",
      "        [-1.3999]], requires_grad=True) \t tensor([0.1999], requires_grad=True)\n",
      "num 86, loss 0.000045\n",
      "tensor([[ 1.1999],\n",
      "        [-1.3996]], requires_grad=True) \t tensor([0.2003], requires_grad=True)\n",
      "num 87, loss 0.000045\n",
      "tensor([[ 1.2001],\n",
      "        [-1.3996]], requires_grad=True) \t tensor([0.2003], requires_grad=True)\n",
      "num 88, loss 0.000045\n",
      "tensor([[ 1.1999],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.1998], requires_grad=True)\n",
      "num 89, loss 0.000046\n",
      "tensor([[ 1.1994],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.2000], requires_grad=True)\n",
      "num 90, loss 0.000045\n",
      "tensor([[ 1.1998],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n",
      "num 91, loss 0.000045\n",
      "tensor([[ 1.2000],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.1999], requires_grad=True)\n",
      "num 92, loss 0.000046\n",
      "tensor([[ 1.1993],\n",
      "        [-1.3998]], requires_grad=True) \t tensor([0.2000], requires_grad=True)\n",
      "num 93, loss 0.000046\n",
      "tensor([[ 1.1995],\n",
      "        [-1.3999]], requires_grad=True) \t tensor([0.2005], requires_grad=True)\n",
      "num 94, loss 0.000045\n",
      "tensor([[ 1.1997],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.2002], requires_grad=True)\n",
      "num 95, loss 0.000045\n",
      "tensor([[ 1.2001],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.2003], requires_grad=True)\n",
      "num 96, loss 0.000046\n",
      "tensor([[ 1.1997],\n",
      "        [-1.3998]], requires_grad=True) \t tensor([0.1996], requires_grad=True)\n",
      "num 97, loss 0.000046\n",
      "tensor([[ 1.1996],\n",
      "        [-1.3995]], requires_grad=True) \t tensor([0.2000], requires_grad=True)\n",
      "num 98, loss 0.000045\n",
      "tensor([[ 1.1997],\n",
      "        [-1.3999]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n",
      "num 99, loss 0.000046\n",
      "tensor([[ 1.2002],\n",
      "        [-1.4003]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n",
      "num 100, loss 0.000045\n",
      "tensor([[ 1.2002],\n",
      "        [-1.3997]], requires_grad=True) \t tensor([0.2001], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for num in range(learn_num):\n",
    "    for i, j in read_minexamples(minnum, x, y):\n",
    "        #求损失\n",
    "        l = loss(line(w, i, b), j)\n",
    "        #从y反向传播\n",
    "        l.sum().backward()\n",
    "        #求梯度\n",
    "        sgd([w, b], learnr, minnum)\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(line(w, x, b), y)\n",
    "    print(f'num {num + 1}, loss {float(train_l.mean()):f}')\n",
    "    print(w,'\\t',b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "123b9932f9da79072ca55f3df90f065e94efe2c192347af71fbcdc6b42c3e4aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
