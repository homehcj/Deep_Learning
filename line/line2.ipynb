{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单层全连接神经网络\n",
    "**pytorch快速实现**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(n,w,b):\n",
    "    X=torch.normal(0,1,(n,len(w)))\n",
    "    y=torch.matmul(X,w) + b\n",
    "    y=y+torch.normal(0,0.01,y.shape)\n",
    "    return X,y.reshape((-1,1))\n",
    "\n",
    "W = torch.tensor([2, -3.4])\n",
    "B = 4.2\n",
    "X,Y = create_data(1000,W, B)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.1380,  1.7124],\n",
       "         [ 0.7528, -0.5059],\n",
       "         [ 0.4959, -0.7707],\n",
       "         [-1.4186, -0.2760],\n",
       "         [ 0.2803, -1.7414],\n",
       "         [-1.7690,  0.6616],\n",
       "         [-1.5222, -0.3153],\n",
       "         [-1.1543,  0.7071],\n",
       "         [ 0.8357,  1.0122],\n",
       "         [-0.4023, -0.3866]]),\n",
       " tensor([[-1.3564],\n",
       "         [ 7.4231],\n",
       "         [ 7.8292],\n",
       "         [ 2.2932],\n",
       "         [10.6669],\n",
       "         [-1.5997],\n",
       "         [ 2.2228],\n",
       "         [-0.5203],\n",
       "         [ 2.4494],\n",
       "         [ 4.7111]])]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#数据集，抽样数量，是否随机抽样\n",
    "def load_array(traindata,num, is_rand=True):\n",
    "    getdata = data.TensorDataset(*traindata)\n",
    "    return data.DataLoader(getdata, num, shuffle=is_rand)\n",
    "get_data=load_array((X,Y),10)\n",
    "#读取数据\n",
    "next(iter(get_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential(nn.Linear(2,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化权重参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.normal_(0,0.01)\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=torch.optim.SGD(net.parameters(),lr=0.03)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学习\n",
    "+ 通过调⽤net(X)⽣成预测并计算损失l（前向传播）。\n",
    "+ 通过进⾏反向传播来计算梯度。\n",
    "+ 通过调⽤优化器来更新模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学习次数:1, 损失0.000207 w= tensor([[ 1.9962, -3.3934]]) b= tensor([4.1941])\n",
      "学习次数:2, 损失0.000110 w= tensor([[ 2.0005, -3.3996]]) b= tensor([4.2008])\n",
      "学习次数:3, 损失0.000110 w= tensor([[ 2.0004, -3.3998]]) b= tensor([4.2007])\n",
      "学习次数:4, 损失0.000110 w= tensor([[ 1.9998, -3.3994]]) b= tensor([4.2003])\n",
      "学习次数:5, 损失0.000110 w= tensor([[ 2.0004, -3.3998]]) b= tensor([4.2000])\n",
      "学习次数:6, 损失0.000111 w= tensor([[ 1.9991, -3.3994]]) b= tensor([4.2007])\n",
      "学习次数:7, 损失0.000110 w= tensor([[ 2.0003, -3.3994]]) b= tensor([4.2005])\n",
      "学习次数:8, 损失0.000110 w= tensor([[ 2.0003, -3.3995]]) b= tensor([4.2004])\n",
      "学习次数:9, 损失0.000111 w= tensor([[ 2.0006, -3.4006]]) b= tensor([4.2005])\n",
      "学习次数:10, 损失0.000112 w= tensor([[ 2.0014, -3.4000]]) b= tensor([4.2006])\n",
      "学习结果:\n",
      "w误差: tensor([[1.4486e-03, 2.3603e-05]]) b误差: tensor([0.0006])\n"
     ]
    }
   ],
   "source": [
    "learnnum = 10\n",
    "for epoch in range(learnnum):\n",
    "    for i, j in get_data:\n",
    "        #计算损失\n",
    "        l = loss(net(i) ,j)\n",
    "        #偏导清零\n",
    "        train.zero_grad()\n",
    "        #反向传播\n",
    "        l.backward()\n",
    "        #梯度下降更新权重\n",
    "        train.step()\n",
    "    l = loss(net(X), Y)\n",
    "    print(f'学习次数:{epoch + 1}, 损失{l:f}','w=',net[0].weight.data,'b=',net[0].bias.data)\n",
    "print('学习结果:')\n",
    "print('w误差:',net[0].weight.data-W,'b误差:',net[0].bias.data-B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
