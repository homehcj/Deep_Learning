{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单层全连接神经网络\n",
    "**pytorch快速实现**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(n,w,b):\n",
    "    X=torch.normal(0,1,(n,len(w)))\n",
    "    y=torch.matmul(X,w) + b\n",
    "    y=y+torch.normal(0,0.01,y.shape)\n",
    "    return X,y.reshape((-1,1))\n",
    "\n",
    "W = torch.tensor([2, -3.4])\n",
    "B = 4.2\n",
    "X,Y = create_data(1000,W, B)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.4368,  1.4494],\n",
       "         [ 0.7026, -1.0230],\n",
       "         [ 1.1767,  2.0859],\n",
       "         [-0.1515,  1.4230],\n",
       "         [ 0.3404,  0.2206],\n",
       "         [ 0.4400,  2.0525],\n",
       "         [-0.1847,  0.4137],\n",
       "         [ 1.0753,  0.0205],\n",
       "         [-0.9192,  0.0132],\n",
       "         [ 0.5390,  2.2246]]),\n",
       " tensor([[ 2.1343],\n",
       "         [ 9.0783],\n",
       "         [-0.5322],\n",
       "         [-0.9434],\n",
       "         [ 4.1408],\n",
       "         [-1.8894],\n",
       "         [ 2.4434],\n",
       "         [ 6.2744],\n",
       "         [ 2.3228],\n",
       "         [-2.3015]])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#数据集，抽样数量，是否随机抽样\n",
    "def load_array(traindata,num, is_rand=True):\n",
    "    getdata = data.TensorDataset(*traindata)\n",
    "    return data.DataLoader(getdata, num, shuffle=is_rand)\n",
    "get_data=load_array((X,Y),10)\n",
    "#读取数据\n",
    "next(iter(get_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential(nn.Linear(2,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化权重参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.normal_(0,0.01)\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=torch.optim.SGD(net.parameters(),lr=0.03)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学习\n",
    "+ 通过调⽤net(X)⽣成预测并计算损失l（前向传播）。\n",
    "+ 通过进⾏反向传播来计算梯度。\n",
    "+ 通过调⽤优化器来更新模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学习次数:1, 损失0.000206 w= tensor([[ 1.9964, -3.3944]]) b= tensor([4.1922])\n",
      "学习次数:2, 损失0.000100 w= tensor([[ 2.0009, -3.3995]]) b= tensor([4.1993])\n",
      "学习次数:3, 损失0.000100 w= tensor([[ 2.0005, -3.3995]]) b= tensor([4.2000])\n",
      "学习次数:4, 损失0.000100 w= tensor([[ 2.0009, -3.4001]]) b= tensor([4.2002])\n",
      "学习次数:5, 损失0.000100 w= tensor([[ 2.0001, -3.3998]]) b= tensor([4.2000])\n",
      "学习次数:6, 损失0.000100 w= tensor([[ 2.0009, -3.4001]]) b= tensor([4.1993])\n",
      "学习次数:7, 损失0.000100 w= tensor([[ 2.0011, -3.3996]]) b= tensor([4.2003])\n",
      "学习次数:8, 损失0.000100 w= tensor([[ 2.0006, -3.3996]]) b= tensor([4.1994])\n",
      "学习次数:9, 损失0.000100 w= tensor([[ 2.0008, -3.4001]]) b= tensor([4.2000])\n",
      "学习次数:10, 损失0.000100 w= tensor([[ 2.0013, -3.3999]]) b= tensor([4.2002])\n",
      "学习结果:\n",
      "w误差: tensor([[1.3070e-03, 6.7949e-05]]) b误差: tensor([0.0002])\n"
     ]
    }
   ],
   "source": [
    "learnnum = 10\n",
    "for epoch in range(learnnum):\n",
    "    for i, j in get_data:\n",
    "        #计算损失\n",
    "        l = loss(net(i) ,j)\n",
    "        #偏导清零\n",
    "        train.zero_grad()\n",
    "        #反向传播\n",
    "        l.backward()\n",
    "        #梯度下降更新权重\n",
    "        train.step()\n",
    "        #显示梯=-\n",
    "        #print(net[0].weight.grad)\n",
    "    l = loss(net(X), Y)\n",
    "    print(f'学习次数:{epoch + 1}, 损失{l:f}','w=',net[0].weight.data,'b=',net[0].bias.data)\n",
    "print('学习结果:')\n",
    "print('w误差:',net[0].weight.data-W,'b误差:',net[0].bias.data-B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
